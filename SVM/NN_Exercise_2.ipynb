{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Networks - Deep Learning**\n",
        "## Exercise 2 - Classification using SVM Models\n",
        "\n",
        "### Georgios Tsoumplekas, AEM: 9359"
      ],
      "metadata": {
        "id": "dqczc2qjouBp"
      },
      "id": "dqczc2qjouBp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 0:** Laying the ground"
      ],
      "metadata": {
        "id": "ccqAzbk8pBI6"
      },
      "id": "ccqAzbk8pBI6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "b7ujyvESpGqL"
      },
      "id": "b7ujyvESpGqL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733614b2",
      "metadata": {
        "id": "733614b2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "\n",
        "import time\n",
        "import random as rnd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f27852",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23f27852",
        "outputId": "1df56f85-ae74-42d0-e394-46f4ebf06be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.7/dist-packages (1.2.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install cvxopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a572b6d",
      "metadata": {
        "id": "3a572b6d"
      },
      "outputs": [],
      "source": [
        "from cvxopt import matrix, solvers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocesing of the MNIST dataset**"
      ],
      "metadata": {
        "id": "5QLmgBdkoq6E"
      },
      "id": "5QLmgBdkoq6E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd7ea99",
      "metadata": {
        "id": "1bd7ea99"
      },
      "outputs": [],
      "source": [
        "#MNIST dataset parameters\n",
        "num_classes = 2 #total classes: odd and even numbers\n",
        "num_features = 784 #data features (28x28 pixels image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "669b8b79",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "669b8b79",
        "outputId": "d03f365a-9648-44ae-8096-93a6af73763e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape (60000, 28, 28)\n",
            "y_train shape (60000,)\n",
            "x_test shape (10000, 28, 28)\n",
            "y_test shape (10000,)\n",
            "y_ train even shape: (29492,)\n",
            "y_ train odd shape: (30508,)\n",
            "y_ test even shape: (4926,)\n",
            "y_ test odd shape: (5074,)\n"
          ]
        }
      ],
      "source": [
        "#Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"x_train shape\", x_train.shape) \n",
        "print(\"y_train shape\", y_train.shape)\n",
        "print(\"x_test shape\", x_test.shape)\n",
        "print(\"y_test shape\", y_test.shape)\n",
        "\n",
        "#Convert to float32\n",
        "x_train, x_test = np.array(x_train, dtype=np.float32), np.array(x_test, dtype=np.float32)\n",
        "y_train, y_test = np.array(y_train, dtype=np.float32), np.array(y_test, dtype=np.float32)\n",
        "\n",
        "#Flatten image to 1-D vector\n",
        "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
        "\n",
        "#Normalize to [0,1]\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "\n",
        "# Change labels of training set: -1 for even numbers, 1 for odd numbers\n",
        "even = np.where(y_train%2 == 0)\n",
        "odd = np.where(y_train%2 == 1)\n",
        "y_train[even] = -1\n",
        "y_train[odd] = 1\n",
        "\n",
        "print(\"y_ train even shape:\", y_train[even].shape)\n",
        "print(\"y_ train odd shape:\", y_train[odd].shape)\n",
        "\n",
        "# Change labels of test set: -1 for even numbers, 1 for odd numbers\n",
        "even = np.where(y_test%2 == 0)\n",
        "odd = np.where(y_test%2 == 1)\n",
        "y_test[even] = -1\n",
        "y_test[odd] = 1\n",
        "\n",
        "print(\"y_ test even shape:\", y_test[even].shape)\n",
        "print(\"y_ test odd shape:\", y_test[odd].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7605c046",
      "metadata": {
        "id": "7605c046"
      },
      "source": [
        "### **Dimensionality reduction using PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9c113e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b9c113e",
        "outputId": "01380c75-f850-4867-b82c-4c4d33d16e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA elapsed time: 7.42681097984314s\n",
            "\n",
            "We extract 87 feautures from the original 784.\n",
            "Cumulative explained variation for 87 principal components: 0.9001063108444214\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "pca = PCA(n_components=0.9).fit(x_train)\n",
        "x_train_pca = pca.transform(x_train)\n",
        "x_test_pca = pca.transform(x_test)\n",
        "\n",
        "end = time.time()\n",
        "print(\"PCA elapsed time: {}s\\n\".format(end-start))\n",
        "print(\"We extract {} feautures from the original {}.\".format(x_train_pca.shape[1],x_train.shape[1]))\n",
        "print('Cumulative explained variation for {} principal components: {}'.format(x_train_pca.shape[1], np.sum(pca.explained_variance_ratio_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal was that the new components (features) achieve 90% explained variance of the initial ones. We see that by creating only 87 of them we can achieve this number (roughly 1/10th of the initial 784 features). "
      ],
      "metadata": {
        "id": "Igb4Xo_6tOgt"
      },
      "id": "Igb4Xo_6tOgt"
    },
    {
      "cell_type": "markdown",
      "id": "8ad168d4",
      "metadata": {
        "id": "8ad168d4"
      },
      "source": [
        "# **Part 1:** SVM implemented with QP solver"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An SVM works by dealing with the classification problem as an optimization problem. More specifically, the training process of an SVM consists of finding the optimal hyperplane that maximizes the margin between this hyperplane and the samples of the 2 classes that lie closer to it (the support vectors). \\\n",
        " This is a well known optimization problem that can be analyzed using Lagrange multipliers and be solved using Quadratic Programming. We can adjust the problem's parameters so that we can allow some of the samples to be misclassified (by adding slack variables) or transform the data to a higher-dimensionality space using kernel functions hoping that they become linearly separable. \\\n",
        " "
      ],
      "metadata": {
        "id": "3cB2vUePtXzF"
      },
      "id": "3cB2vUePtXzF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define kernel functions"
      ],
      "metadata": {
        "id": "f6VPCn5bqRy6"
      },
      "id": "f6VPCn5bqRy6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear kernel works pretty well when the data are (or almost are) linearly separable."
      ],
      "metadata": {
        "id": "LuZ6TEkz2yr2"
      },
      "id": "LuZ6TEkz2yr2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c22f5c",
      "metadata": {
        "id": "88c22f5c"
      },
      "outputs": [],
      "source": [
        "def linear_kernel(x1, x2):\n",
        "    return np.dot(x1, x2.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gaussian kernel can help by transforming the data to a higher-dimensionality space where we hope that the data are linearly separable. Here, we don't actually transform the data to the high-dimensional feature space but instead use the 'kernel trick' to do the necessary calculations in the initial low-dimensional feature space."
      ],
      "metadata": {
        "id": "GOc9OLIq3Ahb"
      },
      "id": "GOc9OLIq3Ahb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e726ad",
      "metadata": {
        "id": "f0e726ad"
      },
      "outputs": [],
      "source": [
        "def gaussian_kernel(x, y, gamma):\n",
        "    return np.exp(-gamma*linalg.norm(x-y)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define SVM model"
      ],
      "metadata": {
        "id": "5oDvCtp2qbsE"
      },
      "id": "5oDvCtp2qbsE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06b7bb2",
      "metadata": {
        "id": "f06b7bb2"
      },
      "outputs": [],
      "source": [
        "class SVM(object):\n",
        "\n",
        "    def __init__(self, kernel=linear_kernel, C=None, gamma=None, disp=True):\n",
        "        self.kernel = kernel\n",
        "        self.C = C\n",
        "        self.gamma = gamma\n",
        "        self.disp = disp\n",
        "        if self.C is not None: \n",
        "            self.C = float(self.C)\n",
        "    \n",
        "    def fit(self, x_train, y_train):        \n",
        "        n_samples, n_features = x_train.shape\n",
        "\n",
        "        if self.disp is True:\n",
        "            print('Number of samples:', n_samples)\n",
        "            print('Number of features:', n_features)\n",
        "\n",
        "        # Calculate Gram matrix\n",
        "        start = time.time()\n",
        "        K = np.zeros((n_samples, n_samples))\n",
        "        for i in range(n_samples):\n",
        "            for j in range(n_samples):\n",
        "                if self.kernel is linear_kernel:\n",
        "                    K[i,j] = self.kernel(x_train[i], x_train[j])\n",
        "                else:\n",
        "                    K[i,j] = self.kernel(x_train[i], x_train[j],self.gamma)\n",
        "        end = time.time()\n",
        "        if self.disp is True:\n",
        "            print(\"Gram matrix elapsed time: {}s\\n\".format(end-start))\n",
        "        \n",
        "        # Formulate the matrices of the QP problem\n",
        "        P = matrix(np.outer(y_train,y_train) * K)\n",
        "        q = matrix(np.ones(n_samples) * -1.0)\n",
        "        A = matrix(y_train.astype('double'), (1,n_samples))\n",
        "        b = matrix(0.0)\n",
        "        \n",
        "        if self.C is None:\n",
        "            G = matrix(np.diag(np.ones(n_samples) * -1))\n",
        "            h = matrix(np.zeros(n_samples))\n",
        "        else:\n",
        "            G_1 = -1*np.identity(n_samples)\n",
        "            G_2 = np.identity(n_samples)\n",
        "            G = matrix(np.vstack((G_1, G_2)))\n",
        "            h_1 = np.zeros(n_samples)\n",
        "            h_2 = np.ones(n_samples) * self.C\n",
        "            h = matrix(np.hstack((h_1, h_2)))\n",
        "          \n",
        "        # Solve QP problem\n",
        "        start = time.time()\n",
        "        solvers.options['show_progress'] = False\n",
        "        solution = solvers.qp(P, q, G, h, A, b, )\n",
        "        end = time.time()\n",
        "\n",
        "        if self.disp is True:\n",
        "            print(\"QP solver elapsed time: {}s\\n\".format(end-start))\n",
        "        \n",
        "        # Lagrange multipliers\n",
        "        a = np.ravel(solution['x'])\n",
        "        \n",
        "        # Find Support Vectors\n",
        "        sv = a > 1e-3\n",
        "        self.a = a[sv]\n",
        "        self.sv = x_train[sv]\n",
        "        self.sv_y = y_train[sv]\n",
        "        ind = np.arange(len(a))[sv] #just a trick to get the indexes which are also stored in sv\n",
        "        \n",
        "        # Bias (calculated for each sv and then we take the mean for numerical stability reasons)\n",
        "        self.b = 0\n",
        "        for n in range(len(self.a)):\n",
        "            self.b += self.sv_y[n]\n",
        "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
        "        self.b /= len(self.a)\n",
        "        \n",
        "    def predict(self, x_test):\n",
        "        start = time.time()\n",
        "        y_pred = np.zeros(len(x_test))\n",
        "        for i in range(len(x_test)):\n",
        "            for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
        "                if self.kernel is linear_kernel:\n",
        "                    y_pred[i] += a * sv_y * self.kernel(x_test[i], sv)\n",
        "                else:\n",
        "                    y_pred[i] += a * sv_y * self.kernel(x_test[i], sv, self.gamma)\n",
        "        y_pred += self.b\n",
        "        end = time.time()\n",
        "        if self.disp is True:\n",
        "            print(\"Testing elapsed time: {}s\\n\".format(end-start))\n",
        "        return np.sign(y_pred).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the main problems of solving the optimization problem using  Quadratic Programming (QP) is that the formulation of the problem requires the knowledge of the whole Gram matrix. However, the Gram matrix can become extremely big when we have to deal with a big training set rendering it unable to fit in the memory. In our example, to demonstrate the SVM model that uses a QP solver and test its performance afterwards, we will train the model in a subset of the original training set."
      ],
      "metadata": {
        "id": "XqnPflog5Jzc"
      },
      "id": "XqnPflog5Jzc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear SVM with C=1"
      ],
      "metadata": {
        "id": "iER_dIuT6cG4"
      },
      "id": "iER_dIuT6cG4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ee421d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ee421d",
        "outputId": "b2923d7e-2f1b-4a2b-972b-16ee8032a422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 2000\n",
            "Number of features: 87\n",
            "Gram matrix elapsed time: 10.590656280517578s\n",
            "\n",
            "QP solver elapsed time: 12.363476991653442s\n",
            "\n",
            "Testing elapsed time: 21.558725833892822s\n",
            "\n",
            "Test set accuracy:  0.8726\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.88      0.86      0.87      4926\n",
            "         1.0       0.87      0.89      0.88      5074\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The training subset consists of 2000 samples\n",
        "rand_indx = np.random.randint(0,len(x_train_pca),2000)\n",
        "x_train_mini = x_train_pca[rand_indx,:]\n",
        "y_train_mini = y_train[rand_indx]\n",
        "\n",
        "# Train the model\n",
        "svm = SVM(kernel=linear_kernel, C=1, gamma=None, disp=True)\n",
        "svm.fit(x_train_mini,y_train_mini)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = svm.predict(x_test_pca)\n",
        "\n",
        "print('Test set accuracy: ', accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, half of the training time was used to calculate the Gram matrix and the other half for solving the optimization problem. Moreover, the model has no problem making predictions over a large test set, but it is clear that inference here is not as fast as in an MLP model per se. As for its performance, we can see that it is pretty good but not excelent. "
      ],
      "metadata": {
        "id": "vvV13-Rz6hHE"
      },
      "id": "vvV13-Rz6hHE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM with RBF kernel: C=1, gamma=0.01"
      ],
      "metadata": {
        "id": "rJTzZni09Iam"
      },
      "id": "rJTzZni09Iam"
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVM(kernel=gaussian_kernel, C=1, gamma=1e-2, disp=True)\n",
        "svm.fit(x_train_mini,y_train_mini)\n",
        "y_pred = svm.predict(x_test_pca)\n",
        "\n",
        "print('Test set accuracy: ', accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa_Y57VB4m5P",
        "outputId": "9ff1fbc1-ee87-4257-f7e2-95e2c1d818ef"
      },
      "id": "Qa_Y57VB4m5P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 2000\n",
            "Number of features: 87\n",
            "Gram matrix elapsed time: 60.9993999004364s\n",
            "\n",
            "QP solver elapsed time: 8.565208911895752s\n",
            "\n",
            "Testing elapsed time: 121.89988827705383s\n",
            "\n",
            "Test set accuracy:  0.9495\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.95      0.95      0.95      4926\n",
            "         1.0       0.95      0.95      0.95      5074\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that calculation of the Gram matrix is 6 times slower for the same training set compared to the linear SVM. However, the optimization problem takes less time to be solved. This is probably due to the fact that the transformed data are probably more linearly separable making the optimization problem easier to solve. Additionaly, the time needed for inference on the test set is also 6 times bigger than before, due to the fact that the test samples have to also pass through the RBF kernel. \\\n",
        "Regarding the performance of the model, we can see that it performs clearly better in comparison with the linear SVM. This leads us to understand that the initial data are not linearly separable but the kernel transformation makes them more linearly separable in the high-dimensional space."
      ],
      "metadata": {
        "id": "HNNTe2WC-hrv"
      },
      "id": "HNNTe2WC-hrv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2:** Hyper-parameter tuning"
      ],
      "metadata": {
        "id": "JdPNIhY65bC2"
      },
      "id": "JdPNIhY65bC2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important aspect of creating an appropriate model for a specific dataset, is to define the optimal hyper-parameters for it. \\\n",
        "In order to find the optimal model we will apply grid search. Each of the candidate models will then be cross-validated using stratified 5-fold cross validation in order to achieve more robust estimations of the accuracy of each model. \\\n",
        "Finaly, the model with the best accuracy is being chosen."
      ],
      "metadata": {
        "id": "7g8MX53EANKi"
      },
      "id": "7g8MX53EANKi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning the linear SVM"
      ],
      "metadata": {
        "id": "I9IUsahG5l-8"
      },
      "id": "I9IUsahG5l-8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of the linear SVM the only hyperparameter that needs to be defined is C. C controls the comprimise we make between the complexity of the model and the number of wrongly classified training samples. High values of parameter C are used when we do not want to allow many misclassified samples while smaller values of C are used when we know that the samples are not linearly separable and it's better to allow more misclassified samples but have a model with greater generalization power."
      ],
      "metadata": {
        "id": "ddBJWz0K6iUT"
      },
      "id": "ddBJWz0K6iUT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a32694",
      "metadata": {
        "id": "a6a32694"
      },
      "outputs": [],
      "source": [
        "def find_best_linear_SVM(k,c_vals,x_train,y_train):\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=k,shuffle=True)\n",
        "\n",
        "    acc_per_fold = np.zeros(k)\n",
        "    best_acc = -1.0\n",
        "    best_C = None\n",
        "\n",
        "    model_no = 1\n",
        "\n",
        "    for i in c_vals:\n",
        "        fold_no = 0\n",
        "\n",
        "        # Create model\n",
        "        svm_model = SVM(kernel=linear_kernel,C=i,disp=False)\n",
        "\n",
        "        for train, test in kfold.split(x_train, y_train):\n",
        "            # Train model\n",
        "            svm_model.fit(x_train[train],y_train[train])\n",
        "\n",
        "            # Evaluate model on validation set\n",
        "            y_pred = svm_model.predict(x_train[test])\n",
        "            acc_per_fold[fold_no] = accuracy_score(y_train[test],y_pred)\n",
        "\n",
        "            fold_no += 1\n",
        "\n",
        "        # Mean of accuracy for 5-fold cross-validation of a specific model\n",
        "        acc = np.mean(acc_per_fold)\n",
        "\n",
        "        print('Model {}:'.format(model_no))\n",
        "        print('C = ', i)\n",
        "        print('Accuracy: ', acc)\n",
        "        print('\\n')\n",
        "\n",
        "        model_no += 1\n",
        "\n",
        "        # Choose best model based on its accuracy\n",
        "        if(acc > best_acc):\n",
        "            best_acc = acc\n",
        "            best_C = i\n",
        "\n",
        "    print('Optimal parameters for the model:')\n",
        "    print('C = ', best_C)\n",
        "    print('Accuracy: ', best_acc)\n",
        "\n",
        "    return best_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0af054c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0af054c",
        "outputId": "f5ea9b55-2c7c-431e-fde3-ff7701f6af20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1:\n",
            "C =  1.0\n",
            "Accuracy:  0.8605\n",
            "\n",
            "\n",
            "Model 2:\n",
            "C =  10.0\n",
            "Accuracy:  0.8614999999999998\n",
            "\n",
            "\n",
            "Model 3:\n",
            "C =  100.0\n",
            "Accuracy:  0.8645000000000002\n",
            "\n",
            "\n",
            "Model 4:\n",
            "C =  1000.0\n",
            "Accuracy:  0.858\n",
            "\n",
            "\n",
            "Model 5:\n",
            "C =  10000.0\n",
            "Accuracy:  0.8539999999999999\n",
            "\n",
            "\n",
            "Optimal parameters for the model:\n",
            "C =  100.0\n",
            "Accuracy:  0.8645000000000002\n"
          ]
        }
      ],
      "source": [
        "# Find best C for the linear SVM\n",
        "k = 5\n",
        "c_vals = np.array([1, 10, 1e2, 1e3, 1e4])\n",
        "best_C = find_best_linear_SVM(k,c_vals,x_train_mini,y_train_mini)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the difference between the accuracy of the different candidate models is pretty small meaning that the performance of the linear SVM is not heavily affected by the choice of C in this particular problem."
      ],
      "metadata": {
        "id": "XvTT_5FqC31b"
      },
      "id": "XvTT_5FqC31b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce798a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ce798a1",
        "outputId": "954a71d2-2c37-47f9-8251-6b3c1a5b1af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 2000\n",
            "Number of features: 87\n",
            "Gram matrix elapsed time: 10.598548412322998s\n",
            "\n",
            "QP solver elapsed time: 17.644381284713745s\n",
            "\n",
            "Testing elapsed time: 21.190078496932983s\n",
            "\n",
            "Test set accuracy:  0.8738\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.88      0.86      0.87      4926\n",
            "         1.0       0.87      0.89      0.88      5074\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define optimal linear SVM\n",
        "svm_model = SVM(kernel=linear_kernel, C=best_C, disp=True)\n",
        "\n",
        "# Train model\n",
        "svm_model.fit(x_train_mini,y_train_mini)\n",
        "\n",
        "# Evaluate model on test set\n",
        "y_pred = svm_model.predict(x_test_pca)\n",
        "\n",
        "print('Test set accuracy: ', accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can verify that the choice of C doesn't seem to play a really important role here from the fact that the optimal model performs only slightly better than the linear SVM that we had in Part 1."
      ],
      "metadata": {
        "id": "MPdwplI4D9q8"
      },
      "id": "MPdwplI4D9q8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning the Gaussian-kernel SVM"
      ],
      "metadata": {
        "id": "F1EjQyTP5xmA"
      },
      "id": "F1EjQyTP5xmA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of the Gaussian-kernel SVM, apart from C we have to also define hyperparameter gamma. Gamma affects the shape of the Gaussian kernel, with bigger values making the shape of the gaussian function more \"flat\" and smaller values of gamma making it more \"steep\"."
      ],
      "metadata": {
        "id": "dKyhYiGwEyF6"
      },
      "id": "dKyhYiGwEyF6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "378e435e",
      "metadata": {
        "id": "378e435e"
      },
      "outputs": [],
      "source": [
        "def find_best_gaussian_SVM(k,c_vals,gamma_vals,x_train,y_train):\n",
        "    kfold = StratifiedKFold(n_splits=k,shuffle=True)\n",
        "\n",
        "    acc_per_fold = np.zeros(k)\n",
        "    best_acc = -1.0\n",
        "    best_C = None\n",
        "    best_gamma = None\n",
        "\n",
        "    model_no = 1\n",
        "    \n",
        "    for i in c_vals:\n",
        "        for j in gamma_vals:\n",
        "            \n",
        "            fold_no = 0\n",
        "\n",
        "            # Create model\n",
        "            svm_model = SVM(kernel=gaussian_kernel, C=i, gamma=j ,disp=False)\n",
        "\n",
        "            for train, test in kfold.split(x_train, y_train):\n",
        "                # Train model\n",
        "                svm_model.fit(x_train[train],y_train[train])\n",
        "\n",
        "                # Evaluate model on validation set\n",
        "                y_pred = svm_model.predict(x_train[test])\n",
        "                acc_per_fold[fold_no] = accuracy_score(y_train[test],y_pred)\n",
        "\n",
        "                fold_no += 1\n",
        "                \n",
        "            # Mean of accuracy for 5-fold cross-validation of a specific model\n",
        "            acc = np.mean(acc_per_fold)\n",
        "\n",
        "            print('Model {}:'.format(model_no))\n",
        "            print('C = ', i)\n",
        "            print('gamma = ', j)\n",
        "            print('Accuracy: ', acc)\n",
        "            print('\\n')\n",
        "\n",
        "            model_no += 1\n",
        "\n",
        "            # Choose best model based on its accuracy\n",
        "            if(acc > best_acc):\n",
        "                best_acc = acc\n",
        "                best_C = i\n",
        "                best_gamma = j\n",
        "                \n",
        "    print('\\nOptimal parameters for the model:')\n",
        "    print('C = ', best_C)\n",
        "    print('gamma = ',best_gamma)\n",
        "    print('Accuracy: ', best_acc)\n",
        "\n",
        "    return best_C, best_gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc90e2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dc90e2b",
        "outputId": "1a06f2b3-d3a7-4e11-984d-3e1c51c4b1eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1:\n",
            "C =  1.0\n",
            "gamma =  0.001\n",
            "Accuracy:  0.8685\n",
            "\n",
            "\n",
            "Model 2:\n",
            "C =  1.0\n",
            "gamma =  0.01\n",
            "Accuracy:  0.945\n",
            "\n",
            "\n",
            "Model 3:\n",
            "C =  1.0\n",
            "gamma =  0.1\n",
            "Accuracy:  0.916\n",
            "\n",
            "\n",
            "Model 4:\n",
            "C =  100.0\n",
            "gamma =  0.001\n",
            "Accuracy:  0.9309999999999998\n",
            "\n",
            "\n",
            "Model 5:\n",
            "C =  100.0\n",
            "gamma =  0.01\n",
            "Accuracy:  0.945\n",
            "\n",
            "\n",
            "Model 6:\n",
            "C =  100.0\n",
            "gamma =  0.1\n",
            "Accuracy:  0.9195\n",
            "\n",
            "\n",
            "Model 7:\n",
            "C =  10000.0\n",
            "gamma =  0.001\n",
            "Accuracy:  0.9450000000000001\n",
            "\n",
            "\n",
            "Model 8:\n",
            "C =  10000.0\n",
            "gamma =  0.01\n",
            "Accuracy:  0.9564999999999999\n",
            "\n",
            "\n",
            "Model 9:\n",
            "C =  10000.0\n",
            "gamma =  0.1\n",
            "Accuracy:  0.9190000000000002\n",
            "\n",
            "\n",
            "\n",
            "Optimal parameters for the model:\n",
            "C =  10000.0\n",
            "gamma =  0.01\n",
            "Accuracy:  0.9564999999999999\n"
          ]
        }
      ],
      "source": [
        "# Find best C and gamma for the gaussian-kernel SVM\n",
        "k = 5\n",
        "c_vals = np.array([1, 1e2, 1e4])\n",
        "gamma_vals = np.array([1e-3, 1e-2, 1e-1])\n",
        "best_C, best_gamma = find_best_gaussian_SVM(k,c_vals,gamma_vals,x_train_mini,y_train_mini)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see again, for constant values of gamma, different values of C affect just slightly the accuracy of the model. However, gamma seems to play an important role with the model performing clearly better when gamma=0.01. More specifically, for that value of gamma, the model performs better when we have a large value of C, too. This can be explained by the fact that for that value of gamma, the transformed data become as linearly separable as they can be in the high-dimensional space and as a result it is better to penalize misclassified samples more."
      ],
      "metadata": {
        "id": "Jq-NFk24G03I"
      },
      "id": "Jq-NFk24G03I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2f7bdf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b2f7bdf",
        "outputId": "6d5ccd70-2436-47a9-f7cb-c9d9e936d4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 2000\n",
            "Number of features: 87\n",
            "Gram matrix elapsed time: 60.61814045906067s\n",
            "\n",
            "QP solver elapsed time: 15.361772775650024s\n",
            "\n",
            "Testing elapsed time: 91.80328059196472s\n",
            "\n",
            "Test set accuracy:  0.9603\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.96      0.96      0.96      4926\n",
            "         1.0       0.96      0.96      0.96      5074\n",
            "\n",
            "    accuracy                           0.96     10000\n",
            "   macro avg       0.96      0.96      0.96     10000\n",
            "weighted avg       0.96      0.96      0.96     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Retrain best Gaussian kernel SVM\n",
        "svm_model = SVM(kernel=gaussian_kernel, C=best_C, gamma=best_gamma ,disp=True)\n",
        "\n",
        "# Train model\n",
        "svm_model.fit(x_train_mini,y_train_mini)\n",
        "\n",
        "# Evaluate model on test set\n",
        "y_pred = svm_model.predict(x_test_pca)\n",
        "\n",
        "print('Test set accuracy: ', accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing execution times with the RBF SVM model we had in Part 1, we can see that different values of gamma don't really seem to affect the time needed to calculate the Gram matrix. However, it took a bit more time here to solve the optimization problem. \\\n",
        "As for the performance of the model, we can see that its accuracy is the highest we have achieved compared with the rest of the models we have tried out so far."
      ],
      "metadata": {
        "id": "wN_EoVpGSPbz"
      },
      "id": "wN_EoVpGSPbz"
    },
    {
      "cell_type": "markdown",
      "id": "1389acd6",
      "metadata": {
        "id": "1389acd6"
      },
      "source": [
        "# **Part 3:** SVM with SMO implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we mentioned earlier, one of the main problems of trying to solve the optimization problem with a QP solver is that we need to calculate the Gram matrix that occurs from the training set samples. However, this requires a lot of memory, especially when the training set is large.\\\n",
        "A way to tackle this is by breaking the original QP problem into smaller QP subproblems that can be solved more easily and the solution of each one of them contributes to the solution of the original problem. \\\n",
        "An algorithm that is based on this logic is Sequential Minimal Optimization (SMO). SMO tries to optimize only two variables at a time. The advantage of this method is that we can calculate the optimal values of these variables analytically so there is no need to use a QP optimizer."
      ],
      "metadata": {
        "id": "3m7Spt9mSzaO"
      },
      "id": "3m7Spt9mSzaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a005649c",
      "metadata": {
        "id": "a005649c"
      },
      "outputs": [],
      "source": [
        "class SVM_SMO():\n",
        "\n",
        "    def __init__(self, max_iter=5000, kernel_type='linear', C=1.0, epsilon=0.001):\n",
        "        self.kernels = {\n",
        "            'linear' : self.kernel_linear,\n",
        "            'quadratic' : self.kernel_quadratic\n",
        "        }\n",
        "        self.max_iter = max_iter\n",
        "        self.kernel_type = kernel_type\n",
        "        self.C = C\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "    def objective_function(self, a, X, y):\n",
        "        # Find  current suport vectors\n",
        "        sv_indx = a > 1e-3\n",
        "        y_sv = y[sv_indx]\n",
        "        x_sv = X[sv_indx,:]\n",
        "        a_sv = a[sv_indx]\n",
        "\n",
        "        if self.kernel_type == 'linear':\n",
        "            result = np.sum(a_sv)-0.5*np.sum((y_sv[:,None]*y_sv[None,:])* \\\n",
        "                            self.kernel_linear(x_sv,x_sv)* \\\n",
        "                            (a_sv[:,None]*a_sv[None,:]))\n",
        "        else:\n",
        "            result = np.sum(a_sv)-0.5*np.sum((y_sv[:,None]*y_sv[None,:])* \\\n",
        "                            self.kernel_quadratic(x_sv,x_sv)* \\\n",
        "                            (a_sv[:,None]*a_sv[None,:])) \n",
        "        return result\n",
        "    \n",
        "    def fit(self, X, y, disp=False):\n",
        "\n",
        "        # Initialization\n",
        "        n, d = X.shape[0], X.shape[1]\n",
        "        alpha = np.zeros((n))\n",
        "        kernel = self.kernels[self.kernel_type]\n",
        "        count = 0\n",
        "        while True:\n",
        "            count += 1\n",
        "            alpha_prev = np.copy(alpha)\n",
        "            for j in range(0, n):\n",
        "                # Choose two variables to optimize\n",
        "                i = self.get_rnd_int(0, n-1, j) # Get random int i~=j\n",
        "                x_i, x_j, y_i, y_j = X[i,:], X[j,:], y[i], y[j]\n",
        "\n",
        "                k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n",
        "                if k_ij == 0:\n",
        "                    continue\n",
        "                alpha_prime_j, alpha_prime_i = alpha[j], alpha[i]\n",
        "\n",
        "                # Compute bounds for the optimized parameters\n",
        "                (L, H) = self.compute_L_H(self.C, alpha_prime_j, alpha_prime_i, y_j, y_i)\n",
        "\n",
        "                # Compute model parameters\n",
        "                self.w = self.calc_w(alpha, y, X)\n",
        "                self.b = self.calc_b(X, y, self.w)\n",
        "\n",
        "                # Compute E_i, E_j\n",
        "                E_i = self.E(x_i, y_i, self.w, self.b)\n",
        "                E_j = self.E(x_j, y_j, self.w, self.b)\n",
        "\n",
        "                # Set new alpha values\n",
        "                alpha[j] = alpha_prime_j + float(y_j * (E_i - E_j))/k_ij\n",
        "                alpha[j] = max(alpha[j], L)\n",
        "                alpha[j] = min(alpha[j], H)\n",
        "\n",
        "                alpha[i] = alpha_prime_i + y_i*y_j * (alpha_prime_j - alpha[j])\n",
        "            \n",
        "            if disp is True:\n",
        "                obj = self.objective_function(alpha,X,y)\n",
        "                print('Iteration: {}, obective function: {}'.format(count,obj))\n",
        "            \n",
        "            # Check convergence\n",
        "            diff = np.linalg.norm(alpha - alpha_prev)\n",
        "            if diff < self.epsilon:\n",
        "                break\n",
        "\n",
        "            if count >= self.max_iter:\n",
        "                print(\"Iteration number exceeded the max of %d iterations\" % (self.max_iter))\n",
        "                return\n",
        "            \n",
        "        # Compute final model parameters\n",
        "        self.b = self.calc_b(X, y, self.w)\n",
        "        if self.kernel_type == 'linear':\n",
        "            self.w = self.calc_w(alpha, y, X)\n",
        "        # Get support vectors\n",
        "        alpha_idx = np.where(alpha > 0)[0]\n",
        "        support_vectors = X[alpha_idx, :]\n",
        "        return support_vectors, count\n",
        "    \n",
        "    def calc_b(self, X, y, w):\n",
        "        b_tmp = y - np.dot(w.T, X.T)\n",
        "        return np.mean(b_tmp)\n",
        "    \n",
        "    def calc_w(self, alpha, y, X):\n",
        "        return np.dot(X.T, np.multiply(alpha,y))\n",
        "    \n",
        "    # Prediction (used in the training step)\n",
        "    def h(self, X, w, b):\n",
        "        return np.sign(np.dot(w.T, X.T) + b).astype(int)\n",
        "    \n",
        "    # Prediction (used in testing)\n",
        "    def predict_test(self,x_test):\n",
        "        return np.sign(np.dot(self.w.T, x_test.T) + self.b).astype(int)\n",
        "    \n",
        "    # Prediction error\n",
        "    def E(self, x_k, y_k, w, b):\n",
        "        return self.h(x_k, w, b) - y_k\n",
        "    \n",
        "    # Computes bounds within which the optimized variables lie\n",
        "    def compute_L_H(self, C, alpha_prime_j, alpha_prime_i, y_j, y_i):\n",
        "        if(y_i != y_j):\n",
        "            return (max(0, alpha_prime_j - alpha_prime_i), min(C, C - alpha_prime_i + alpha_prime_j))\n",
        "        else:\n",
        "            return (max(0, alpha_prime_i + alpha_prime_j - C), min(C, alpha_prime_i + alpha_prime_j))\n",
        "        \n",
        "    def get_rnd_int(self, a,b,z):\n",
        "        i = z\n",
        "        cnt=0\n",
        "        while i == z and cnt<1000:\n",
        "            i = rnd.randint(a,b)\n",
        "            cnt=cnt+1\n",
        "        return i\n",
        "    \n",
        "    # Define kernels\n",
        "    def kernel_linear(self, x1, x2):\n",
        "        return np.dot(x1, x2.T)\n",
        "    def kernel_quadratic(self, x1, x2):\n",
        "        return (np.dot(x1, x2.T) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimally, we have to apply grid-search and cross-validation as previously in order to find the best values for C and epsilon. Epsilon works as a tolerance bound: when the norm of alphas (the variables which we optimize) changes less than epsilon we stop training. "
      ],
      "metadata": {
        "id": "pqjO6AI_mwXL"
      },
      "id": "pqjO6AI_mwXL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480fc5e2",
      "metadata": {
        "id": "480fc5e2"
      },
      "outputs": [],
      "source": [
        "eps = 1e-3\n",
        "Ce = 10\n",
        "\n",
        "# Define a linear SVM with SMO implementation\n",
        "svm_smo = SVM_SMO(C=Ce, epsilon=eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model with 5000 samples and evaluate it on 1000 samples"
      ],
      "metadata": {
        "id": "OvE7ePgAngiN"
      },
      "id": "OvE7ePgAngiN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809e5c7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "809e5c7d",
        "outputId": "5045db44-354c-4156-c965-e5d0159654ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 3312.116627216339s\n",
            "\n",
            "Testing time: 0.00036144256591796875s\n",
            "\n",
            "Training set accuracy:  0.891\n",
            "Test set accuracy:  0.856\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.88      0.84      0.86       540\n",
            "         1.0       0.83      0.87      0.85       460\n",
            "\n",
            "    accuracy                           0.86      1000\n",
            "   macro avg       0.86      0.86      0.86      1000\n",
            "weighted avg       0.86      0.86      0.86      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a subset of the training set with 5000 samples\n",
        "rand_indx = np.random.randint(0,len(x_train_pca),5000)\n",
        "x_train_mini = x_train_pca[rand_indx,:]\n",
        "y_train_mini = y_train[rand_indx]\n",
        "\n",
        "# Train model\n",
        "start = time.time()\n",
        "svm_smo.fit(x_train_mini,y_train_mini)\n",
        "end = time.time()\n",
        "print(\"Training time: {}s\\n\".format(end-start))\n",
        "\n",
        "# Create a subset of test set with 1000 samples\n",
        "rand_indx = np.random.randint(0,len(x_test_pca),1000)\n",
        "x_test_mini = x_test_pca[rand_indx,:]\n",
        "y_test_mini = y_test[rand_indx]\n",
        "\n",
        "# Evaluate model\n",
        "start = time.time()\n",
        "y_pred_test = svm_smo.predict_test(x_test_mini)\n",
        "end = time.time()\n",
        "print(\"Testing time: {}s\\n\".format(end-start))\n",
        "\n",
        "y_pred_train = svm_smo.predict_test(x_train_mini)\n",
        "\n",
        "print('Training set accuracy: ', accuracy_score(y_train_mini,y_pred_train))\n",
        "print('Test set accuracy: ', accuracy_score(y_test_mini,y_pred_test))\n",
        "\n",
        "print(classification_report(y_test_mini, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is clear that the time needed to train an SVM with SMO has increased dramatically. However, this is a necessary compromise we have to make when we cannot solve the problem with an optimized QP solver due to lack of memory. It is worth noticing that inference is extremely quick due to the fact that we have already calculated w and b in the training process so only a dot product needs to be calculated now.\\\n",
        "Regarding the performance of the model, we can see that it's just slightly worse than that of the linear SVM we had in Part 1 (could be because the training set was smaller here)."
      ],
      "metadata": {
        "id": "PlESjmW-oGPO"
      },
      "id": "PlESjmW-oGPO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model on 20000 samples and evaluate it on 10000 samples"
      ],
      "metadata": {
        "id": "t1Ot894ZnxKJ"
      },
      "id": "t1Ot894ZnxKJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1437be04",
      "metadata": {
        "id": "1437be04"
      },
      "outputs": [],
      "source": [
        "rand_indx = np.random.randint(0,len(x_train_pca),20000)\n",
        "x_train_mini = x_train_pca[rand_indx,:]\n",
        "y_train_mini = y_train[rand_indx]\n",
        "\n",
        "start = time.time()\n",
        "svm_smo.fit(x_train_mini,y_train_mini)\n",
        "end = time.time()\n",
        "print(\"Training time: {}s\\n\".format(end-start))\n",
        "\n",
        "start = time.time()\n",
        "y_pred_test = svm_smo.predict_test(x_test_pca)\n",
        "end = time.time()\n",
        "print(\"Testing time: {}s\\n\".format(end-start))\n",
        "\n",
        "y_pred_train = svm_smo.predict_test(x_train_pca)\n",
        "\n",
        "print('Training set accuracy: ', accuracy_score(y_train,y_pred_train))\n",
        "print('Test set accuracy: ', accuracy_score(y_test,y_pred_test))\n",
        "\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, I was unable to train the SVM with such a big training set because of the extremely long time needed for the training to complete (it was taking more than 10 hours and Google Colab timed-out). However, my prediction is that if it was executed we could expect slightly better performance than the previous case due to the fact that the SVM would have seen more samples in the training process thus making it able to generalize better."
      ],
      "metadata": {
        "id": "ay966U_JDGVT"
      },
      "id": "ay966U_JDGVT"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "NN_Exercise_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}